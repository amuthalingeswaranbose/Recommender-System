{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_manager.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amuthalingeswaranbose/Recommender-System/blob/master/data_manager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PKeiy8TreQJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Created on Nov 9, 2015\n",
        "@author: donghyun\n",
        "'''\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cPickle as pickl\n",
        "import numpy as np\n",
        "\n",
        "from operator import itemgetter\n",
        "from scipy.sparse.csr import csr_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "class Data_Factory():\n",
        "\n",
        "    def load(self, path):\n",
        "        R = pickl.load(open(path + \"/ratings.all\", \"rb\"))\n",
        "        print \"Load preprocessed rating data - %s\" % (path + \"/ratings.all\")\n",
        "        D_all = pickl.load(open(path + \"/document.all\", \"rb\"))\n",
        "        print \"Load preprocessed document data - %s\" % (path + \"/document.all\")\n",
        "        return R, D_all\n",
        "\n",
        "    def save(self, path, R, D_all):\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        print \"Saving preprocessed rating data - %s\" % (path + \"/ratings.all\")\n",
        "        pickl.dump(R, open(path + \"/ratings.all\", \"wb\"))\n",
        "        print \"Done!\"\n",
        "        print \"Saving preprocessed document data - %s\" % (path + \"/document.all\")\n",
        "        pickl.dump(D_all, open(path + \"/document.all\", \"wb\"))\n",
        "        print \"Done!\"\n",
        "\n",
        "    def read_rating(self, path):\n",
        "        results = []\n",
        "        if os.path.isfile(path):\n",
        "            raw_ratings = open(path, 'r')\n",
        "        else:\n",
        "            print \"Path (preprocessed) is wrong!\"\n",
        "            sys.exit()\n",
        "        index_list = []\n",
        "        rating_list = []\n",
        "        all_line = raw_ratings.read().splitlines()\n",
        "        for line in all_line:\n",
        "            tmp = line.split()\n",
        "            num_rating = int(tmp[0])\n",
        "            if num_rating > 0:\n",
        "                tmp_i, tmp_r = zip(*(elem.split(\":\") for elem in tmp[1::]))\n",
        "                index_list.append(np.array(tmp_i, dtype=int))\n",
        "                rating_list.append(np.array(tmp_r, dtype=float))\n",
        "            else:\n",
        "                index_list.append(np.array([], dtype=int))\n",
        "                rating_list.append(np.array([], dtype=float))\n",
        "\n",
        "        results.append(index_list)\n",
        "        results.append(rating_list)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def read_pretrained_word2vec(self, path, vocab, dim):\n",
        "        if os.path.isfile(path):\n",
        "            raw_word2vec = open(path, 'r')\n",
        "        else:\n",
        "            print \"Path (word2vec) is wrong!\"\n",
        "            sys.exit()\n",
        "\n",
        "        word2vec_dic = {}\n",
        "        all_line = raw_word2vec.read().splitlines()\n",
        "        mean = np.zeros(dim)\n",
        "        count = 0\n",
        "        for line in all_line:\n",
        "            tmp = line.split()\n",
        "            _word = tmp[0]\n",
        "            _vec = np.array(tmp[1:], dtype=float)\n",
        "            if _vec.shape[0] != dim:\n",
        "                print \"Mismatch the dimension of pre-trained word vector with word embedding dimension!\"\n",
        "                sys.exit()\n",
        "            word2vec_dic[_word] = _vec\n",
        "            mean = mean + _vec\n",
        "            count = count + 1\n",
        "\n",
        "        mean = mean / count\n",
        "\n",
        "        W = np.zeros((len(vocab) + 1, dim))\n",
        "        count = 0\n",
        "        for _word, i in vocab:\n",
        "            if word2vec_dic.has_key(_word):\n",
        "                W[i + 1] = word2vec_dic[_word]\n",
        "                count = count + 1\n",
        "            else:\n",
        "                W[i + 1] = np.random.normal(mean, 0.1, size=dim)\n",
        "\n",
        "        print \"%d words exist in the given pretrained model\" % count\n",
        "\n",
        "        return W\n",
        "\n",
        "    def split_data(self, ratio, R):\n",
        "        print \"Randomly splitting rating data into training set (%.1f) and test set (%.1f)...\" % (1 - ratio, ratio)\n",
        "        train = []\n",
        "        for i in xrange(R.shape[0]):\n",
        "            user_rating = R[i].nonzero()[1]\n",
        "            np.random.shuffle(user_rating)\n",
        "            train.append((i, user_rating[0]))\n",
        "\n",
        "        remain_item = set(xrange(R.shape[1])) - set(zip(*train)[1])\n",
        "\n",
        "        for j in remain_item:\n",
        "            item_rating = R.tocsc().T[j].nonzero()[1]\n",
        "            np.random.shuffle(item_rating)\n",
        "            train.append((item_rating[0], j))\n",
        "\n",
        "        rating_list = set(zip(R.nonzero()[0], R.nonzero()[1]))\n",
        "        total_size = len(rating_list)\n",
        "        remain_rating_list = list(rating_list - set(train))\n",
        "        random.shuffle(remain_rating_list)\n",
        "\n",
        "        num_addition = int((1 - ratio) * total_size) - len(train)\n",
        "        if num_addition < 0:\n",
        "            print 'this ratio cannot be handled'\n",
        "            sys.exit()\n",
        "        else:\n",
        "            train.extend(remain_rating_list[:num_addition])\n",
        "            tmp_test = remain_rating_list[num_addition:]\n",
        "            random.shuffle(tmp_test)\n",
        "            valid = tmp_test[::2]\n",
        "            test = tmp_test[1::2]\n",
        "\n",
        "            trainset_u_idx, trainset_i_idx = zip(*train)\n",
        "            trainset_u_idx = set(trainset_u_idx)\n",
        "            trainset_i_idx = set(trainset_i_idx)\n",
        "            if len(trainset_u_idx) != R.shape[0] or len(trainset_i_idx) != R.shape[1]:\n",
        "                print \"Fatal error in split function. Check your data again or contact authors\"\n",
        "                sys.exit()\n",
        "\n",
        "        print \"Finish constructing training set and test set\"\n",
        "        return train, valid, test\n",
        "\n",
        "    def generate_train_valid_test_file_from_R(self, path, R, ratio):\n",
        "        '''\n",
        "        Split randomly rating matrix into training set, valid set and test set with given ratio (valid+test)\n",
        "        and save three data sets to given path.\n",
        "        Note that the training set contains at least a rating on every user and item.\n",
        "        Input:\n",
        "        - path: path to save training set, valid set, test set\n",
        "        - R: rating matrix (csr_matrix)\n",
        "        - ratio: (1-ratio), ratio/2 and ratio/2 of the entire dataset (R) will be training, valid and test set, respectively\n",
        "        '''\n",
        "        train, valid, test = self.split_data(ratio, R)\n",
        "        print \"Save training set and test set to %s...\" % path\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        R_lil = R.tolil()\n",
        "        user_ratings_train = {}\n",
        "        item_ratings_train = {}\n",
        "        for i, j in train:\n",
        "            if user_ratings_train.has_key(i):\n",
        "                user_ratings_train[i].append(j)\n",
        "            else:\n",
        "                user_ratings_train[i] = [j]\n",
        "\n",
        "            if item_ratings_train.has_key(j):\n",
        "                item_ratings_train[j].append(i)\n",
        "            else:\n",
        "                item_ratings_train[j] = [i]\n",
        "\n",
        "        user_ratings_valid = {}\n",
        "        item_ratings_valid = {}\n",
        "        for i, j in valid:\n",
        "            if user_ratings_valid.has_key(i):\n",
        "                user_ratings_valid[i].append(j)\n",
        "            else:\n",
        "                user_ratings_valid[i] = [j]\n",
        "\n",
        "            if item_ratings_valid.has_key(j):\n",
        "                item_ratings_valid[j].append(i)\n",
        "            else:\n",
        "                item_ratings_valid[j] = [i]\n",
        "\n",
        "        user_ratings_test = {}\n",
        "        item_ratings_test = {}\n",
        "        for i, j in test:\n",
        "            if user_ratings_test.has_key(i):\n",
        "                user_ratings_test[i].append(j)\n",
        "            else:\n",
        "                user_ratings_test[i] = [j]\n",
        "\n",
        "            if item_ratings_test.has_key(j):\n",
        "                item_ratings_test[j].append(i)\n",
        "            else:\n",
        "                item_ratings_test[j] = [i]\n",
        "\n",
        "        f_train_user = open(path + \"/train_user.dat\", \"w\")\n",
        "        f_valid_user = open(path + \"/valid_user.dat\", \"w\")\n",
        "        f_test_user = open(path + \"/test_user.dat\", \"w\")\n",
        "\n",
        "        formatted_user_train = []\n",
        "        formatted_user_valid = []\n",
        "        formatted_user_test = []\n",
        "\n",
        "        for i in xrange(R.shape[0]):\n",
        "            if user_ratings_train.has_key(i):\n",
        "                formatted = [str(len(user_ratings_train[i]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (j, R_lil[i, j])\n",
        "                                  for j in sorted(user_ratings_train[i])])\n",
        "                formatted_user_train.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_user_train.append(\"0\")\n",
        "\n",
        "            if user_ratings_valid.has_key(i):\n",
        "                formatted = [str(len(user_ratings_valid[i]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (j, R_lil[i, j])\n",
        "                                  for j in sorted(user_ratings_valid[i])])\n",
        "                formatted_user_valid.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_user_valid.append(\"0\")\n",
        "\n",
        "            if user_ratings_test.has_key(i):\n",
        "                formatted = [str(len(user_ratings_test[i]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (j, R_lil[i, j])\n",
        "                                  for j in sorted(user_ratings_test[i])])\n",
        "                formatted_user_test.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_user_test.append(\"0\")\n",
        "\n",
        "        f_train_user.write(\"\\n\".join(formatted_user_train))\n",
        "        f_valid_user.write(\"\\n\".join(formatted_user_valid))\n",
        "        f_test_user.write(\"\\n\".join(formatted_user_test))\n",
        "\n",
        "        f_train_user.close()\n",
        "        f_valid_user.close()\n",
        "        f_test_user.close()\n",
        "        print \"\\ttrain_user.dat, valid_user.dat, test_user.dat files are generated.\"\n",
        "\n",
        "        f_train_item = open(path + \"/train_item.dat\", \"w\")\n",
        "        f_valid_item = open(path + \"/valid_item.dat\", \"w\")\n",
        "        f_test_item = open(path + \"/test_item.dat\", \"w\")\n",
        "\n",
        "        formatted_item_train = []\n",
        "        formatted_item_valid = []\n",
        "        formatted_item_test = []\n",
        "\n",
        "        for j in xrange(R.shape[1]):\n",
        "            if item_ratings_train.has_key(j):\n",
        "                formatted = [str(len(item_ratings_train[j]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (i, R_lil[i, j])\n",
        "                                  for i in sorted(item_ratings_train[j])])\n",
        "                formatted_item_train.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_item_train.append(\"0\")\n",
        "\n",
        "            if item_ratings_valid.has_key(j):\n",
        "                formatted = [str(len(item_ratings_valid[j]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (i, R_lil[i, j])\n",
        "                                  for i in sorted(item_ratings_valid[j])])\n",
        "                formatted_item_valid.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_item_valid.append(\"0\")\n",
        "\n",
        "            if item_ratings_test.has_key(j):\n",
        "                formatted = [str(len(item_ratings_test[j]))]\n",
        "                formatted.extend([\"%d:%.1f\" % (i, R_lil[i, j])\n",
        "                                  for i in sorted(item_ratings_test[j])])\n",
        "                formatted_item_test.append(\" \".join(formatted))\n",
        "            else:\n",
        "                formatted_item_test.append(\"0\")\n",
        "\n",
        "        f_train_item.write(\"\\n\".join(formatted_item_train))\n",
        "        f_valid_item.write(\"\\n\".join(formatted_item_valid))\n",
        "        f_test_item.write(\"\\n\".join(formatted_item_test))\n",
        "\n",
        "        f_train_item.close()\n",
        "        f_valid_item.close()\n",
        "        f_test_item.close()\n",
        "        print \"\\ttrain_item.dat, valid_item.dat, test_item.dat files are generated.\"\n",
        "\n",
        "        print \"Done!\"\n",
        "\n",
        "    def generate_CTRCDLformat_content_file_from_D_all(self, path, D_all):\n",
        "        '''\n",
        "        Write word index with word count in document for CTR&CDL experiment\n",
        "        '''\n",
        "        f_text = open(path + \"mult.dat\", \"w\")\n",
        "        X = D_all['X_base']\n",
        "        formatted_text = []\n",
        "        for i in xrange(X.shape[0]):\n",
        "            word_count = sorted(set(X[i].nonzero()[1]))\n",
        "            formatted = [str(len(word_count))]\n",
        "            formatted.extend([\"%d:%d\" % (j, X[i, j]) for j in word_count])\n",
        "            formatted_text.append(\" \".join(formatted))\n",
        "\n",
        "        f_text.write(\"\\n\".join(formatted_text))\n",
        "        f_text.close()\n",
        "\n",
        "    def preprocess(self, path_rating, path_itemtext, min_rating,\n",
        "                   _max_length, _max_df, _vocab_size):\n",
        "        '''\n",
        "        Preprocess rating and document data.\n",
        "        Input:\n",
        "            - path_rating: path for rating data (data format - user_id::item_id::rating)\n",
        "            - path_itemtext: path for review or synopsis data (data format - item_id::text1|text2|text3|....)\n",
        "            - min_rating: users who have less than \"min_rating\" ratings will be removed (default = 1)\n",
        "            - _max_length: maximum length of document of each item (default = 300)\n",
        "            - _max_df: terms will be ignored that have a document frequency higher than the given threshold (default = 0.5)\n",
        "            - vocab_size: vocabulary size (default = 8000)\n",
        "        Output:\n",
        "            - R: rating matrix (csr_matrix: row - user, column - item)\n",
        "            - D_all['X_sequence']: list of sequence of word index of each item ([[1,2,3,4,..],[2,3,4,...],...])\n",
        "            - D_all['X_vocab']: list of tuple (word, index) in the given corpus\n",
        "        '''\n",
        "        # Validate data paths\n",
        "        if os.path.isfile(path_rating):\n",
        "            raw_ratings = open(path_rating, 'r')\n",
        "            print \"Path - rating data: %s\" % path_rating\n",
        "        else:\n",
        "            print \"Path(rating) is wrong!\"\n",
        "            sys.exit()\n",
        "\n",
        "        if os.path.isfile(path_itemtext):\n",
        "            raw_content = open(path_itemtext, 'r')\n",
        "            print \"Path - document data: %s\" % path_itemtext\n",
        "        else:\n",
        "            print \"Path(item text) is wrong!\"\n",
        "            sys.exit()\n",
        "\n",
        "        # 1st scan document file to filter items which have documents\n",
        "        tmp_id_plot = set()\n",
        "        all_line = raw_content.read().splitlines()\n",
        "        for line in all_line:\n",
        "            tmp = line.split('::')\n",
        "            i = tmp[0]\n",
        "            tmp_plot = tmp[1].split('|')\n",
        "            if tmp_plot[0] == '':\n",
        "                continue\n",
        "            tmp_id_plot.add(i)\n",
        "        raw_content.close()\n",
        "\n",
        "        print \"Preprocessing rating data...\"\n",
        "        print \"\\tCounting # ratings of each user and removing users having less than %d ratings...\" % min_rating\n",
        "        # 1st scan rating file to check # ratings of each user\n",
        "        all_line = raw_ratings.read().splitlines()\n",
        "        tmp_user = {}\n",
        "        for line in all_line:\n",
        "            tmp = line.split('::')\n",
        "            u = tmp[0]\n",
        "            i = tmp[1]\n",
        "            if (i in tmp_id_plot):\n",
        "                if (u not in tmp_user):\n",
        "                    tmp_user[u] = 1\n",
        "                else:\n",
        "                    tmp_user[u] = tmp_user[u] + 1\n",
        "\n",
        "        raw_ratings.close()\n",
        "\n",
        "        # 2nd scan rating file to make matrix indices of users and items\n",
        "        # with removing users and items which are not satisfied with the given\n",
        "        # condition\n",
        "        raw_ratings = open(path_rating, 'r')\n",
        "        all_line = raw_ratings.read().splitlines()\n",
        "        userset = {}\n",
        "        itemset = {}\n",
        "        user_idx = 0\n",
        "        item_idx = 0\n",
        "\n",
        "        user = []\n",
        "        item = []\n",
        "        rating = []\n",
        "\n",
        "        for line in all_line:\n",
        "            tmp = line.split('::')\n",
        "            u = tmp[0]\n",
        "            if u not in tmp_user:\n",
        "                continue\n",
        "            i = tmp[1]\n",
        "            # An user will be skipped where the number of ratings of the user\n",
        "            # is less than min_rating.\n",
        "            if tmp_user[u] >= min_rating:\n",
        "                if u not in userset:\n",
        "                    userset[u] = user_idx\n",
        "                    user_idx = user_idx + 1\n",
        "\n",
        "                if (i not in itemset) and (i in tmp_id_plot):\n",
        "                    itemset[i] = item_idx\n",
        "                    item_idx = item_idx + 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if u in userset and i in itemset:\n",
        "                u_idx = userset[u]\n",
        "                i_idx = itemset[i]\n",
        "\n",
        "                user.append(u_idx)\n",
        "                item.append(i_idx)\n",
        "                rating.append(float(tmp[2]))\n",
        "\n",
        "        raw_ratings.close()\n",
        "\n",
        "        R = csr_matrix((rating, (user, item)))\n",
        "\n",
        "        print \"Finish preprocessing rating data - # user: %d, # item: %d, # ratings: %d\" % (R.shape[0], R.shape[1], R.nnz)\n",
        "\n",
        "        # 2nd scan document file to make idx2plot dictionary according to\n",
        "        # indices of items in rating matrix\n",
        "        print \"Preprocessing item document...\"\n",
        "\n",
        "        # Read Document File\n",
        "        raw_content = open(path_itemtext, 'r')\n",
        "        max_length = _max_length\n",
        "        map_idtoplot = {}\n",
        "        all_line = raw_content.read().splitlines()\n",
        "        for line in all_line:\n",
        "            tmp = line.split('::')\n",
        "            if tmp[0] in itemset:\n",
        "                i = itemset[tmp[0]]\n",
        "                tmp_plot = tmp[1].split('|')\n",
        "                eachid_plot = (' '.join(tmp_plot)).split()[:max_length]\n",
        "                map_idtoplot[i] = ' '.join(eachid_plot)\n",
        "\n",
        "        print \"\\tRemoving stop words...\"\n",
        "        print \"\\tFiltering words by TF-IDF score with max_df: %.1f, vocab_size: %d\" % (_max_df, _vocab_size)\n",
        "\n",
        "        # Make vocabulary by document\n",
        "        vectorizer = TfidfVectorizer(max_df=_max_df, stop_words={\n",
        "                                     'english'}, max_features=_vocab_size)\n",
        "        Raw_X = [map_idtoplot[i] for i in range(R.shape[1])]\n",
        "        vectorizer.fit(Raw_X)\n",
        "        vocab = vectorizer.vocabulary_\n",
        "        X_vocab = sorted(vocab.items(), key=itemgetter(1))\n",
        "\n",
        "        # Make input for run\n",
        "        X_sequence = []\n",
        "        for i in range(R.shape[1]):\n",
        "            X_sequence.append(\n",
        "                [vocab[word] + 1 for word in map_idtoplot[i].split() if vocab.has_key(word)])\n",
        "\n",
        "        '''Make input for CTR & CDL'''\n",
        "        baseline_vectorizer = CountVectorizer(vocabulary=vocab)\n",
        "        X_base = baseline_vectorizer.fit_transform(Raw_X)\n",
        "\n",
        "        D_all = {\n",
        "            'X_sequence': X_sequence,\n",
        "            'X_base': X_base,\n",
        "            'X_vocab': X_vocab,\n",
        "        }\n",
        "\n",
        "        print \"Finish preprocessing document data!\"\n",
        "\n",
        "        return R, D_all"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}